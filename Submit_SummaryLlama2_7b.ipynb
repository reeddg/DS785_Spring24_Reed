{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtAobP5++G0IP6ZMymluQO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "David Reed DS785 Spring 24\n",
        "\n",
        "Summarization Llama2-7b\n",
        "\n",
        " Set Runtime Type to T4 GPU at a minimum\n"
      ],
      "metadata": {
        "id": "Fnw2PXSQDCps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some problems with latest version of transformers so revert to version 4.31"
      ],
      "metadata": {
        "id": "kZpdGIbBdQB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl peft accelerate bitsandbytes datasets\n",
        "!pip install -q xformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers@v4.31-release\n",
        "!pip install -q torch\n",
        "!pip install -q evaluate\n",
        "!pip install -q huggingface_hub huggingface\n",
        "!pip install -q rouge_score\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i9r7phPU2d7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "#!pip list"
      ],
      "metadata": {
        "id": "FlKEV7J6ig66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    )\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        "    )\n",
        "from trl import SFTTrainer\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import importlib.util\n",
        "import pathlib\n",
        "import importlib_metadata\n",
        "import tensorflow as tf\n",
        "import evaluate\n",
        "%load_ext tensorboard\n",
        "import huggingface_hub\n",
        "from datetime import datetime\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "HRRRIw_Dnu5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU and usage."
      ],
      "metadata": {
        "id": "uJWdF0Yuc523"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PlDhvYsPAUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "3_JHzMQCkDQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time=datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
        "str_time = str(time)\n",
        "str_time"
      ],
      "metadata": {
        "id": "s4_GFVD4NXY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the llama2 7b from hugging face\n",
        "model_base = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# name for fine tuned model\n",
        "model_fine_tuned = f\"sumtuned4retail{str_time}\"\n",
        "\n",
        "# dataset directory\n",
        "dataset_dir = \"reeddg/retail_products_chatgtp\"\n",
        "\n",
        "#output directiry\n",
        "dir_out = f\"reeddg/training_outputs/sum_{str_time}\"\n",
        "\n",
        "runs_log_dir = f\"./{dir_out}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "2GGuMvZa3Dlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"\\n\", model_base, \"\\n\", model_fine_tuned,\"\\n\", dataset_dir, \"\\n\", dir_out,\"\\n\", runs_log_dir)"
      ],
      "metadata": {
        "id": "H-SfeeOqNUCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset from hugging face\n",
        "dataset = load_dataset(dataset_dir)\n",
        "# set review sentiment to class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D76Vm38qm5TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c8cZmRVhSF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The code below is to create the Summary text to prepare for training.\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bpZ0MQg1SGk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dft = dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "T9z2aGeLSd8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft.describe()\n"
      ],
      "metadata": {
        "id": "bGa0yHo8TboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft.head()"
      ],
      "metadata": {
        "id": "A43X_G-GUDpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"Description\"][0]"
      ],
      "metadata": {
        "id": "rO06m1EzUHu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"Query\"] = dft.apply(lambda x : x['Category'] + \" \" + x['Description'] + \" \" +  x['User Review'], axis=1)\n",
        "\n",
        "#dft[\"text\"] = dft[[\"Category\", \"Product Name\", \"User Review\"]].apply(lambda x: \"### Instruction:\\n\" + x[\"Category\"] + \" \" + x[\"User Review\"] + \" \\n\\n### Response:\\n\" + x[\"Product Name\"] + \"\\n\", axis=1)\n",
        "dft['Response'] = dft['Summary']"
      ],
      "metadata": {
        "id": "hazocjmNUsk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"{dft['Query'][0]}\")"
      ],
      "metadata": {
        "id": "QujlpoJ9c4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6e4CaXEm3xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[['Query','Description','Response']]"
      ],
      "metadata": {
        "id": "SLYvBfoSasvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"text\"] = dft[[\"Query\", \"Response\"]].apply(lambda x: \"###Query\\n\" + x[\"Query\"] + \" \\n###Response\\n\" + x[\"Response\"] + \" \\n\", axis=1)\n",
        "\n",
        "df = dft[['Query', 'Response', 'text']].copy()\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.3)"
      ],
      "metadata": {
        "id": "G9UPkCQFkiU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "xKsHSTNFtYx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "x0udOhwXrSa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]"
      ],
      "metadata": {
        "id": "h47YvO7n6tzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "b3tfW7GBAcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "alCEtcY9ohYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "#Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
        "\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\""
      ],
      "metadata": {
        "id": "nDiy77tyDyki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "bvjUCguPulgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "        r=8, lora_alpha=16, lora_dropout=0.05, task_type=\"CAUSAL_LM\", bias=\"none\"\n",
        "    )"
      ],
      "metadata": {
        "id": "o263XKAUAiix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config"
      ],
      "metadata": {
        "id": "PcKpdYQOFdSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForCausalLM.from_pretrained(model_base, quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "\n",
        "\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "YECFmrYK2ZsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NsautVwGm4Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-NOvrYrc_AEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Evaluation"
      ],
      "metadata": {
        "id": "OXwuHXNWDSXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy evaluation\n",
        "rouge = evaluate.load(\"rouge\")\n"
      ],
      "metadata": {
        "id": "AsAubOkYDgLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD in PEFT MODEL"
      ],
      "metadata": {
        "id": "aYeyfpN_sNu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_dI2Xy04ZGVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with our model\n",
        "#Input Prompt\n",
        "prompt = \"Detergent Tide Detergent has a fresh clean scent and it cuts through grease. Tide Detergent cleans well\"\n",
        "#Wrap the prompt using the right chat template\n",
        "#instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "# or \"###Query\\n\" + x[\"Query\"] + \" \\n###Response\\n\" + x[\"Response\"] + \" \\n\"\n",
        "instruction = f\"###Query\\n{prompt} \\n###Response\\n\"\n",
        "#Using Pipeline from the hugging face\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
        "result = pipe(instruction)\n",
        "#Trim the response, remove instruction manually\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "id": "39rYHwrL79eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_uuI5-o978mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir= runs_log_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        save_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        num_train_epochs=10,\n",
        "        report_to=\"tensorboard\",\n",
        "        max_steps=-1,\n",
        "        push_to_hub=True,\n",
        "        #evaluation_strategy=\"steps\",\n",
        "        warmup_steps = 10,\n",
        "        fp16=True,\n",
        "        eval_steps=1000,\n",
        "        #load_best_model_at_end=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "zdvoXAIYBEKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "g4npsQXSB1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "05XJ2KwRbPxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset['train'])"
      ],
      "metadata": {
        "id": "tRHzgQJObUNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset['test'][0]\n"
      ],
      "metadata": {
        "id": "OM5B8jVkbFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_arguments,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        "        #data_collator=data_collator,\n",
        "        #compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "metadata": {
        "id": "ulddTH99BI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
        "\n",
        "gpus\n",
        "\n",
        "\n",
        "visible_devices = tf.config.get_visible_devices()\n",
        "for devices in visible_devices:\n",
        "  print(devices)\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "bpwbUNitndWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vR_V8vYNC-Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "f\"{dir_out}\""
      ],
      "metadata": {
        "id": "GvTgaJimVccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "#huggingface_hub.upload_folder\n",
        "#/content/reeddg/training_outputs3/runs/Mar24_14-57-33_ecd96771620a\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"$dir_out\"\n"
      ],
      "metadata": {
        "id": "xmZW-5iKMtEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model on HuggingFace first"
      ],
      "metadata": {
        "id": "i4aRtg47OXMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save fine-tuned model\n",
        "ft_model_path = f\"reeddg/test_train/sum_{model_fine_tuned}\"\n",
        "trainer.model.save_pretrained(ft_model_path)\n",
        "trainer.save_model(ft_model_path)\n",
        "trainer.push_to_hub(ft_model_path)\n"
      ],
      "metadata": {
        "id": "yIZJEkuSDNLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload base model back in\n",
        "base_model= AutoModelForCausalLM.from_pretrained(model_base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0})\n",
        "\n"
      ],
      "metadata": {
        "id": "5J5y5cZKD0xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge base model and the fine-tuned model\n",
        "merged_model= PeftModel.from_pretrained(base_model, ft_model_path)\n",
        "merged_model= merged_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "z_rA7QXxEiA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_base, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "lim17Skw_1vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save merged model\n",
        "merged_model_path= f\"reeddg/test_model/mergedsum_{model_fine_tuned}\"\n",
        "merged_model.save_pretrained(merged_model_path)"
      ],
      "metadata": {
        "id": "ots2EPrTEsKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"reeddg/new_llama2_summ\")\n",
        "tokenizer.push_to_hub(\"reeddg/new_llama2_summ\")"
      ],
      "metadata": {
        "id": "_mfqcrngb-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with our model\n",
        "#Input Prompt\n",
        "prompt = \"Detergent Tide Detergent has a fresh clean scent and it cuts through grease. Tide Detergent cleans well\"\n",
        "#Wrap the prompt using the right chat template\n",
        "#instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "# or \"###Query\\n\" + x[\"Query\"] + \" \\n###Response\\n\" + x[\"Response\"] + \" \\n\"\n",
        "instruction = f\"###Query\\n{prompt} \\n###Response\\n\"\n",
        "#Using Pipeline from the hugging face\n",
        "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=512)\n",
        "result = pipe(instruction)\n",
        "#Trim the response, remove instruction manually\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "id": "CF4S26T3fKnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "torch.cuda.memory_allocated()\n"
      ],
      "metadata": {
        "id": "imBs-5djDoQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForCausalLM.from_pretrained(model_base,\n",
        "                                                      device_map={\"\": 0},\n",
        "                                                      quantization_config=bnb_config,\n",
        "                                                      )"
      ],
      "metadata": {
        "id": "CC7iyeoBh0nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\"reeddg/new_llama2_summ\", use_fast=True)\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "def gen(model,p, maxlen=100, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    #res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cuda')\n",
        "    res = model.generate(**toks.to(\"cuda\"),max_new_tokens=maxlen,num_return_sequences=1,penalty_alpha=0.6,do_sample = True,\n",
        "      top_k=5,temperature=0.5,repetition_penalty=1.2,).to('cuda')\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uiB-v7vsIZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "qty_to_test=len(dataset['test'])\n",
        "dialogues = dataset['test'][0:qty_to_test]['Query']\n",
        "human_baseline_summaries = dataset['test'][0:qty_to_test]['Response']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "time_to_respond_orig = []\n",
        "time_to_respond_peft = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "    #prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
        "\n",
        "    prompt = f\"###Query\\n{dialogue} \\n###Response\\n\"\n",
        "    print(prompt)\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "    original_model_res = gen(original_model,prompt,500,)\n",
        "    #print(original_model_res)\n",
        "    original_model_text_output = original_model_res[0].split('Response\\n')[1]\n",
        "    #print(original_model_text_output)\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_orig.append(out_time)\n",
        "    print(f\"Time taken for original inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "    peft_model_res = gen(merged_model,prompt,500,)\n",
        "    #print(peft_model_res)\n",
        "    peft_model_output = peft_model_res[0].split('Response\\n')[1]\n",
        "    #print(peft_model_output)\n",
        "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_peft.append(out_time)\n",
        "    print(f\"Time taken for peft inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
        "zipped_time = list(zip(time_to_respond_orig, time_to_respond_peft))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['baseline_summary', 'original_model_summary', 'peft_model_summary'])\n",
        "df_time = pd.DataFrame(zipped_time, columns = ['orig_model_inference_time', 'peft_model_inference_time'])\n",
        "df"
      ],
      "metadata": {
        "id": "03nNUMJYG9-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_res[0].split('Response\\n')[1]"
      ],
      "metadata": {
        "id": "3FnB_eBYZtsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_res[0].split('Response\\n')[1]\n"
      ],
      "metadata": {
        "id": "poe29J_gKpx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[dialogues, human_baseline_summaries]"
      ],
      "metadata": {
        "id": "0aBrua-FdDPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "id": "DekiRv6xPVyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "peft_abs_imp_results=dict(zip(peft_model_results.keys(), improvement))\n",
        "for key, value in peft_abs_imp_results.items():\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "yusyZQY3Paoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Inference time peft: {round(np.average(time_to_respond_peft),4)}\")\n",
        "print (f\"Inference time original: {round(np.average(time_to_respond_orig),4)}\")\n",
        "\n",
        "time_p2o = np.subtract(time_to_respond_peft, time_to_respond_orig)\n",
        "time_2inf = np.average(time_p2o)\n",
        "time_p=[\"Time2Imp\"]\n",
        "#print(time_p)\n",
        "time_p = [*time_p, *time_p2o]\n",
        "#print (time_p)\n",
        "\n",
        "\n",
        "if(time_2inf>0):\n",
        "  print(f\"Inference time for original model is on average {round(time_2inf,2)} seconds better than the peft model.\")\n",
        "else:\n",
        "  print(f\"Inference time for peft model is on average {round(-time_2inf,2)} seconds better than the original model.\")"
      ],
      "metadata": {
        "id": "RIlcfqmOzIQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ret_aray(r,rge_dict):\n",
        "   outputs=[r]\n",
        "   for k,v in rge_dict.items():\n",
        "    #print(k,v)\n",
        "    outputs.append(v)\n",
        "   return(outputs)\n",
        "\n",
        "def ret_arayk(r,rge_dict):\n",
        "   outputs=[r]\n",
        "   for k,v in rge_dict.items():\n",
        "    #print(k,v)\n",
        "    outputs.append(k)\n",
        "   return(outputs)\n"
      ],
      "metadata": {
        "id": "zaEZU8dBvafo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "file= model_fine_tuned + \"summary.csv\"\n",
        "\n",
        "with open(file, 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(ret_arayk(\"col\", original_model_results))\n",
        "  print(original_model_results)\n",
        "  writer.writerow(ret_aray(\"orig\",original_model_results))\n",
        "  print(peft_model_results)\n",
        "  writer.writerow(ret_aray(\"peft\",peft_model_results))\n",
        "  print(peft_abs_imp_results)\n",
        "  writer.writerow(ret_aray(\"peft-orig\",peft_abs_imp_results))\n",
        "  print(time_p)\n",
        "  writer.writerow(time_p)\n",
        "file.close()\n",
        "\n",
        "\n",
        "\n",
        "file_time= \"time_\" + model_fine_tuned + \"summary.csv\"\n",
        "\n",
        "df_time.to_csv(file_time,encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "t3Ggtw-6SZM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}