{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOMbKQ2Dup2MRXanR3TBILX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "David Reed DS785 Spring 24\n",
        "\n",
        "Summarization FLAN T5 xxl\n",
        "\n",
        " Set Runtime Type to T4 GPU for the lowest cost.\n",
        "\n",
        " Or to V100 High RAM\n",
        "\n",
        " Or to A100 High RAM\n",
        "\n",
        " but compute units are much more for the higher GPU, higher RAM options.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fnw2PXSQDCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! lscpu"
      ],
      "metadata": {
        "id": "PnXYaZ25o8hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some problems with latest version of transformers so revert to version 4.31"
      ],
      "metadata": {
        "id": "kZpdGIbBdQB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl peft accelerate bitsandbytes datasets\n",
        "!pip install -q xformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers@v4.31-release\n",
        "#!pip install torch\n",
        "!pip install -q evaluate\n",
        "!pip install -q huggingface_hub huggingface\n",
        "!pip install -q rouge_score\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i9r7phPU2d7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "#!pip list"
      ],
      "metadata": {
        "id": "FlKEV7J6ig66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    T5Tokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Trainer,\n",
        "    )\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        "    )\n",
        "from trl import SFTTrainer\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import importlib.util\n",
        "import pathlib\n",
        "import importlib_metadata\n",
        "import tensorflow as tf\n",
        "import evaluate\n",
        "%load_ext tensorboard\n",
        "import huggingface_hub\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "HRRRIw_Dnu5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU and usage."
      ],
      "metadata": {
        "id": "uJWdF0Yuc523"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PlDhvYsPAUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "3_JHzMQCkDQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time=datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
        "str_time = str(time)\n",
        "str_time"
      ],
      "metadata": {
        "id": "s4_GFVD4NXY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the flan xxl from hugging face\n",
        "model_base =  \"google/flan-t5-xxl\"\n",
        "\n",
        "\n",
        "# name for fine tuned model\n",
        "model_fine_tuned = f\"drFlanBase_{str_time}\"\n",
        "\n",
        "# dataset directory\n",
        "dataset_dir = \"reeddg/retail_products_chatgtp\"\n",
        "\n",
        "#output directiry\n",
        "dir_out = f\"reeddg/training_outputs/flan_sum_{str_time}\"\n",
        "\n",
        "runs_log_dir = f\"./{dir_out}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "2GGuMvZa3Dlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"\\n\", model_base, \"\\n\", model_fine_tuned,\"\\n\", dataset_dir, \"\\n\", dir_out,\"\\n\", runs_log_dir)"
      ],
      "metadata": {
        "id": "H-SfeeOqNUCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When accessing Hugging Face you need to log in with Hugging Face cli or use the Secret Token.\n",
        "This uses the HF_TOKEN and granting colab access, then it works through the rest of the code."
      ],
      "metadata": {
        "id": "ApspajlaX5NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset from hugging face\n",
        "dataset = load_dataset(dataset_dir)\n",
        "# set review sentiment to class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D76Vm38qm5TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The code below is to create the Summary text to prepare for training.\n",
        "---\n"
      ],
      "metadata": {
        "id": "bpZ0MQg1SGk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dft = dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "T9z2aGeLSd8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft.describe()\n"
      ],
      "metadata": {
        "id": "bGa0yHo8TboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft.head()"
      ],
      "metadata": {
        "id": "A43X_G-GUDpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"Description\"][0]"
      ],
      "metadata": {
        "id": "rO06m1EzUHu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"Query\"] = dft.apply(lambda x : x['Category'] + \" \" + x['Description'] + \" \" +  x['User Review'], axis=1)\n",
        "\n",
        "#dft[\"text\"] = dft[[\"Category\", \"Product Name\", \"User Review\"]].apply(lambda x: \"### Instruction:\\n\" + x[\"Category\"] + \" \" + x[\"User Review\"] + \" \\n\\n### Response:\\n\" + x[\"Product Name\"] + \"\\n\", axis=1)\n",
        "dft['Response'] = dft['Summary']"
      ],
      "metadata": {
        "id": "hazocjmNUsk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"{dft['Query'][0]}\")"
      ],
      "metadata": {
        "id": "QujlpoJ9c4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft[['Query','Description','Response']]"
      ],
      "metadata": {
        "id": "SLYvBfoSasvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set format for the prompt and answer that the model can use"
      ],
      "metadata": {
        "id": "UFiYz3_xqgdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Flan T5 query"
      ],
      "metadata": {
        "id": "e9bqRdeq6crg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dft[\"question\"] = dft[[\"Query\"]].apply(lambda x: \"summarize: \" + x[\"Query\"] , axis=1)\n",
        "dft[\"labels\"] = dft[\"Response\"]\n",
        "\n",
        "df = dft[['question', 'labels']].copy()\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.3)"
      ],
      "metadata": {
        "id": "JTcIxKaU57Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "xKsHSTNFtYx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "x0udOhwXrSa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]"
      ],
      "metadata": {
        "id": "h47YvO7n6tzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "b3tfW7GBAcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "alCEtcY9ohYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "#Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_base)\n",
        "\n",
        "tokenizer.pad_token= tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "nDiy77tyDyki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"question\"], truncation=True), batched=True)\n",
        "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"labels\"], truncation=True), batched=True)\n",
        "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
        "print(f\"Max target length: {max_target_length}\")"
      ],
      "metadata": {
        "id": "3C7iseQ2nA4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = dataset['train'][0]\n",
        "inputs = examples[\"question\"]\n",
        "model_inputs = tokenizer(inputs, max_length=68, truncation=True, padding=True)\n",
        "model_inputs"
      ],
      "metadata": {
        "id": "-nfZnOO4HIL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
        "    # The \"inputs\" are the tokenized answer:\n",
        "    inputs = examples[\"question\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=68, truncation=True, padding=True)\n",
        "\n",
        "    # The \"labels\" are the tokenized outputs:\n",
        "    labels = tokenizer(text_target=examples[\"labels\"], max_length=32, truncation=True, padding=True)\n",
        "\n",
        "\n",
        "    #labels[\"input_ids\"] = [\n",
        "    #    [(l if l != tokenizer.pad_token_id else 00) for l in label] for label in labels[\"input_ids\"]\n",
        "    #]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Map the preprocessing function across the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['question'])"
      ],
      "metadata": {
        "id": "WG_tMUGiCSpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['test'][0]['labels']"
      ],
      "metadata": {
        "id": "v7JacD9F4trA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    input_text = tokenizer.decode(tokenized_dataset['test'][0]['input_ids'], skip_special_tokens=True)\n",
        "    output_text = np.where(tokenized_dataset['test']['labels']!= -100, tokenized_dataset['test'][0]['labels'], tokenizer.pad_token_id)\n",
        "    output_text = tokenizer.decode(tokenized_dataset['test'][0]['labels'], skip_special_tokens=True)\n",
        "    #label_text = tokenizer.decode(tokenized_dataset['test'][0]['question'], skip_special_tokens=True)\n",
        "\n",
        "    print('Input:', input_text)\n",
        "    print('Output:', output_text)\n",
        "    #print('Label:', label_text)\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "3jitv05e4DRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "bvjUCguPulgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, task_type=\"SEQ_2_SEQ_LM\", bias=\"none\"\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "o263XKAUAiix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config"
      ],
      "metadata": {
        "id": "PcKpdYQOFdSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_base, quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "\n",
        "\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "YECFmrYK2ZsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "U1Ogv_kSDXt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-NOvrYrc_AEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Evaluation"
      ],
      "metadata": {
        "id": "OXwuHXNWDSXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy evaluation\n",
        "rouge = evaluate.load(\"rouge\")\n"
      ],
      "metadata": {
        "id": "AsAubOkYDgLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pad_token_id = tokenizer.eos_token_id\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    return_tensors =\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "mW8iSoFklm6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD in PEFT MODEL"
      ],
      "metadata": {
        "id": "aYeyfpN_sNu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_dI2Xy04ZGVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with our model\n",
        "#Input Prompt\n",
        "prompt = \"Detergent Tide Detergent has a fresh clean scent and it cuts through grease. Tide Detergent cleans well\"\n",
        "#Wrap the prompt using the right chat template\n",
        "\n",
        "instruction = f\"summarize: {prompt}\"\n",
        "print(instruction)\n",
        "pipe = pipeline(task=\"summarization\", model=model, tokenizer=tokenizer, max_length=32)\n",
        "result = pipe(instruction)\n",
        "#Trim the response, remove instruction manually\n",
        "print(result[0]['summary_text'])"
      ],
      "metadata": {
        "id": "39rYHwrL79eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Generation works despite the warning. Noted."
      ],
      "metadata": {
        "id": "_uuI5-o978mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = Seq2SeqTrainingArguments(\n",
        "        output_dir= runs_log_dir,\n",
        "        learning_rate=2e-4,\n",
        "        save_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        num_train_epochs=10,\n",
        "        generation_max_length=32,\n",
        "        report_to=\"tensorboard\",\n",
        "        max_steps=-1,\n",
        "        push_to_hub=True,\n",
        "        eval_steps=1000,\n",
        "    )"
      ],
      "metadata": {
        "id": "zdvoXAIYBEKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "g4npsQXSB1PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "05XJ2KwRbPxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset['train'])"
      ],
      "metadata": {
        "id": "tRHzgQJObUNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset['test'][0]\n"
      ],
      "metadata": {
        "id": "OM5B8jVkbFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_arguments,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        #peft_config=peft_config,\n",
        "        #dataset_text_field=\"text\",\n",
        "        #max_seq_length=68,\n",
        "        data_collator=data_collator,\n",
        "        #compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "metadata": {
        "id": "ulddTH99BI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now look at the GPU's, not required for evaluation, but to see GPU and CPU since colab has different options want to make sure GPU available."
      ],
      "metadata": {
        "id": "7u25tcWKcIKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
        "\n",
        "gpus\n",
        "\n",
        "\n",
        "visible_devices = tf.config.get_visible_devices()\n",
        "for devices in visible_devices:\n",
        "  print(devices)\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "bpwbUNitndWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vR_V8vYNC-Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "f\"{dir_out}\""
      ],
      "metadata": {
        "id": "GvTgaJimVccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "#huggingface_hub.upload_folder\n",
        "#/content/reeddg/training_outputs3/runs/Mar24_14-57-33_ecd96771620a\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"$dir_out\"\n"
      ],
      "metadata": {
        "id": "xmZW-5iKMtEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model on HuggingFace first"
      ],
      "metadata": {
        "id": "i4aRtg47OXMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save fine-tuned model\n",
        "ft_model_path = f\"reeddg/test_train/sum_{model_fine_tuned}\"\n",
        "trainer.model.save_pretrained(ft_model_path)\n",
        "trainer.save_model(ft_model_path)\n",
        "trainer.push_to_hub(ft_model_path)\n"
      ],
      "metadata": {
        "id": "yIZJEkuSDNLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload base model back in\n",
        "base_model= AutoModelForSeq2SeqLM.from_pretrained(model_base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0})\n",
        "\n"
      ],
      "metadata": {
        "id": "5J5y5cZKD0xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge base model and the fine-tuned model\n",
        "merged_model= PeftModel.from_pretrained(base_model, ft_model_path)\n",
        "merged_model= merged_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "z_rA7QXxEiA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_base, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "#tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "lim17Skw_1vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save merged model\n",
        "merged_model_path= f\"reeddg/test_model/mergedsum_{model_fine_tuned}\"\n",
        "merged_model.save_pretrained(merged_model_path)"
      ],
      "metadata": {
        "id": "ots2EPrTEsKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"reeddg/T5sumxxl\")\n",
        "tokenizer.push_to_hub(\"reeddg/T5sumxxl\")"
      ],
      "metadata": {
        "id": "_mfqcrngb-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run text generation pipeline with our model\n",
        "#Input Prompt\n",
        "prompt = \"Detergent Tide Detergent has a fresh clean scent and it cuts through grease. Tide Detergent cleans well\"\n",
        "#Wrap the prompt using the right chat template\n",
        "\n",
        "instruction = f\"summarize: {prompt}\"\n",
        "print(instruction)\n",
        "#Using Pipeline from the hugging face\n",
        "pipe = pipeline(task=\"summarization\", model=merged_model, tokenizer=tokenizer, max_length=32)\n",
        "\n",
        "result = pipe(instruction)\n",
        "#Trim the response, remove instruction manually\n",
        "#print(result[0]['summary_text'][len(instruction):].split('Response\\n')[0])\n",
        "print(result[0]['summary_text'])"
      ],
      "metadata": {
        "id": "CF4S26T3fKnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "torch.cuda.memory_allocated()\n"
      ],
      "metadata": {
        "id": "imBs-5djDoQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUT OF GPU RAM, so restart and reload merged model from Hugging Face, Load the data, load the tokenizer and continue."
      ],
      "metadata": {
        "id": "X7rGyc1FsKWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use this to load in the model later, after it has been stored to Hugging Face"
      ],
      "metadata": {
        "id": "ixrZXTS8fjh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = AutoModelForSeq2SeqLM.from_pretrained(\"reeddg/T5sumxxl\", quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})"
      ],
      "metadata": {
        "id": "V6s73uHT8DU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_base,\n",
        "                                                      device_map={\"\": 0},\n",
        "                                                      quantization_config=bnb_config,\n",
        "                                                      )"
      ],
      "metadata": {
        "id": "CC7iyeoBh0nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\"reeddg/T5sumxxl\", use_fast=True)\n",
        "#eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "def gen(model,p, maxlen=68, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    #res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cuda')\n",
        "    res = model.generate(**toks.to(\"cuda\"),max_new_tokens=maxlen,num_return_sequences=1,penalty_alpha=0.6,do_sample = True,\n",
        "      top_k=5,temperature=0.5,repetition_penalty=1.2,).to('cuda')\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uiB-v7vsIZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "UvLR5Y-wOKVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset['test'][0]\n"
      ],
      "metadata": {
        "id": "S8mNiP_qXRKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diEFKuQKgqZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "qty_to_test=len(dataset['test'])\n",
        "dialogues = dataset['test'][0:qty_to_test]['question']\n",
        "human_baseline_summaries = dataset['test'][0:qty_to_test]['labels']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "time_to_respond_orig = []\n",
        "time_to_respond_peft = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
        "\n",
        "    prompt = f\"summarize: {dialogue}\"\n",
        "    print(prompt)\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "    original_model_res = gen(original_model,prompt,68)\n",
        "    print(original_model_res)\n",
        "    original_model_text_output = original_model_res[0]#.split('Response\\n')[1]\n",
        "    print(original_model_text_output)\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_orig.append(out_time)\n",
        "    print(f\"Time taken for original inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "    peft_model_res = gen(merged_model,prompt,32)\n",
        "    print(peft_model_res)\n",
        "    peft_model_output = peft_model_res[0]#.split('Response\\n')[1]\n",
        "    print(peft_model_output)\n",
        "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_peft.append(out_time)\n",
        "    print(f\"Time taken for peft inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
        "zipped_time = list(zip(time_to_respond_orig, time_to_respond_peft))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['baseline_summary', 'original_model_summary', 'peft_model_summary'])\n",
        "df_time = pd.DataFrame(zipped_time, columns = ['orig_model_inference_time', 'peft_model_inference_time'])\n",
        "df"
      ],
      "metadata": {
        "id": "v67BAnrFgr3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_res[0]"
      ],
      "metadata": {
        "id": "3FnB_eBYZtsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_res[0]\n"
      ],
      "metadata": {
        "id": "poe29J_gKpx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[dialogues, human_baseline_summaries]"
      ],
      "metadata": {
        "id": "0aBrua-FdDPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "id": "DekiRv6xPVyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "peft_abs_imp_results=dict(zip(peft_model_results.keys(), improvement))\n",
        "for key, value in peft_abs_imp_results.items():\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "yusyZQY3Paoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Inference time peft: {round(np.average(time_to_respond_peft),4)}\")\n",
        "print (f\"Inference time original: {round(np.average(time_to_respond_orig),4)}\")\n",
        "\n",
        "time_p2o = np.subtract(time_to_respond_peft, time_to_respond_orig)\n",
        "time_2inf = np.average(time_p2o)\n",
        "time_p=[\"Time2Imp\"]\n",
        "#print(time_p)\n",
        "time_p = [*time_p, *time_p2o]\n",
        "#print (time_p)\n",
        "\n",
        "\n",
        "if(time_2inf>0):\n",
        "  print(f\"Inference time for original model is on average {round(time_2inf,2)} seconds better than the peft model.\")\n",
        "else:\n",
        "  print(f\"Inference time for peft model is on average {round(-time_2inf,2)} seconds better than the original model.\")"
      ],
      "metadata": {
        "id": "RIlcfqmOzIQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ret_aray(r,rge_dict):\n",
        "   outputs=[r]\n",
        "   for k,v in rge_dict.items():\n",
        "    #print(k,v)\n",
        "    outputs.append(v)\n",
        "   return(outputs)\n",
        "\n",
        "def ret_arayk(r,rge_dict):\n",
        "   outputs=[r]\n",
        "   for k,v in rge_dict.items():\n",
        "    #print(k,v)\n",
        "    outputs.append(k)\n",
        "   return(outputs)\n"
      ],
      "metadata": {
        "id": "zaEZU8dBvafo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file= model_fine_tuned + \"summary.csv\"\n",
        "\n",
        "with open(file, 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  writer.writerow(ret_arayk(\"col\", original_model_results))\n",
        "  print(original_model_results)\n",
        "  writer.writerow(ret_aray(\"orig\",original_model_results))\n",
        "  print(peft_model_results)\n",
        "  writer.writerow(ret_aray(\"peft\",peft_model_results))\n",
        "  print(peft_abs_imp_results)\n",
        "  writer.writerow(ret_aray(\"peft-orig\",peft_abs_imp_results))\n",
        "  print(time_p)\n",
        "  writer.writerow(time_p)\n",
        "file.close()\n",
        "\n",
        "\n",
        "\n",
        "file_time= \"time_\" + model_fine_tuned + \"summary.csv\"\n",
        "\n",
        "df_time.to_csv(file_time,encoding='utf-8')\n",
        "\n"
      ],
      "metadata": {
        "id": "Zv6RVOsf5GF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_fine_tuned)\n"
      ],
      "metadata": {
        "id": "gxNNfWuxyyr3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}