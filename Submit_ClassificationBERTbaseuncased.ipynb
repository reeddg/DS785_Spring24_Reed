{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOozcVwLF06J4hXfKRRYYVy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "David Reed DS785 Spring 24\n",
        "\n",
        "Classification BERT base uncased\n",
        "\n",
        " Set Runtime Type to T4 GPU for the lowest cost.\n",
        "\n",
        " Or to V100 High RAM\n",
        "\n",
        " Or to A100 High RAM\n",
        "\n",
        " but compute units are much more for the higher GPU, higher RAM options.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fnw2PXSQDCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! lscpu"
      ],
      "metadata": {
        "id": "C6JxJoBMTME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some problems with latest version of transformers so revert to version 4.31"
      ],
      "metadata": {
        "id": "kZpdGIbBdQB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl peft accelerate bitsandbytes datasets\n",
        "!pip install -q git+https://github.com/huggingface/transformers@v4.31-release\n",
        "!pip install -q torch\n",
        "!pip install -q evaluate\n",
        "!pip install -q huggingface_hub huggingface\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i9r7phPU2d7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "#!pip list"
      ],
      "metadata": {
        "id": "FlKEV7J6ig66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    T5Tokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Trainer,\n",
        "    BertTokenizer,\n",
        "    BertModel,\n",
        "    )\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        "    )\n",
        "from trl import SFTTrainer\n",
        "import sklearn\n",
        "import matplotlib\n",
        "import importlib.util\n",
        "import pathlib\n",
        "import importlib_metadata\n",
        "import tensorflow as tf\n",
        "import evaluate\n",
        "%load_ext tensorboard\n",
        "import huggingface_hub\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "tqt20iG-Tdk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU and usage."
      ],
      "metadata": {
        "id": "uJWdF0Yuc523"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PlDhvYsPAUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "3_JHzMQCkDQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time=datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
        "str_time = str(time)\n",
        "str_time"
      ],
      "metadata": {
        "id": "s4_GFVD4NXY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model from hugging face\n",
        "model_base = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "\n",
        "# name for fine tuned model\n",
        "model_fine_tuned = f\"classBERTBase{str_time}\"\n",
        "\n",
        "# dataset directory\n",
        "dataset_dir = \"reeddg/retail_products_chatgtp\"\n",
        "\n",
        "#output directiry\n",
        "dir_out = f\"reeddg/training_outputs/{str_time}\"\n",
        "\n",
        "runs_log_dir = f\"./{dir_out}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "2GGuMvZa3Dlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (model_base, \"\\n\", model_fine_tuned,\"\\n\", dataset_dir, \"\\n\", dir_out,\"\\n\", runs_log_dir)"
      ],
      "metadata": {
        "id": "H-SfeeOqNUCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset from hugging face\n",
        "dataset = load_dataset(dataset_dir)\n",
        "# set review sentiment to class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D76Vm38qm5TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dft = dataset[\"train\"].to_pandas()\n",
        "#dft[\"text\"] = dft[[\"Category\", \"Product Name\", \"User Review\"]].apply(lambda x: x[\"Category\"] + \" \" + x[\"User Review\"] + \" \" + x[\"Product Name\"] , axis=1)\n",
        "dft[\"text\"] = dft[\"User Review\"]\n",
        "dft[\"labels\"] = dft[\"Review Sentiment\"]\n",
        "df = dft[['text' , 'labels']].copy()\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.class_encode_column(\"labels\")\n",
        "# stratify to have positive and negative reviews\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.3, stratify_by_column=\"labels\")"
      ],
      "metadata": {
        "id": "f8nzcJEv3y79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n"
      ],
      "metadata": {
        "id": "hRE6A0uT6aeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"]"
      ],
      "metadata": {
        "id": "h47YvO7n6tzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfr = dataset[\"train\"].to_pandas()\n",
        "counts = dfr['labels'].value_counts()\n",
        "print(\"Train\\n\", counts)\n",
        "dfr = dataset[\"test\"].to_pandas()\n",
        "counts = dfr['labels'].value_counts()\n",
        "print(\"Test\\n\", counts)"
      ],
      "metadata": {
        "id": "NQn2rCp8-XMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "b3tfW7GBAcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "alCEtcY9ohYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "#Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True, add_prefix_space=True)\n"
      ],
      "metadata": {
        "id": "nDiy77tyDyki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT add pad token if none exists\n",
        "  #if tokenizer.pad_token is None:\n",
        "   # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    # model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "GN2c6jgqajYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # extract text\n",
        "    text = examples[\"text\"]\n",
        "\n",
        "    #tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "VYGYl7zlaqg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize training and validation datasets\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "U-S-qdHgayAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "0aCDCGyQDBAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator"
      ],
      "metadata": {
        "id": "Brs2LtrfDIM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "# define label maps\n",
        "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
        "label2id = {\"Negative\":0, \"Positive\":1}\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=2, id2label=id2label, label2id=label2id, quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "\n",
        "\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "YECFmrYK2ZsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NsautVwGm4Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-NOvrYrc_AEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Evaluation"
      ],
      "metadata": {
        "id": "OXwuHXNWDSXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy evaluation\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "f1 = evaluate.load(\"f1\")"
      ],
      "metadata": {
        "id": "AsAubOkYDgLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval fxn for trainer\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    a = accuracy.compute(predictions=predictions, references=labels)\n",
        "    r = recall.compute(predictions=predictions, references=labels)\n",
        "    p = precision.compute(predictions=predictions, references=labels)\n",
        "    f = f1.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": a['accuracy'], \"precision\": p['precision'], \"recall\": r['recall'], \"f1\": f['f1']}"
      ],
      "metadata": {
        "id": "FHWPL9e1EFVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define list of examples\n",
        "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
        "\n",
        "print(\"Untrained model predictions:\")\n",
        "print(\"----------------------------\")\n",
        "for text in text_list:\n",
        "    # tokenize text\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    # compute logits\n",
        "    logits = model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions = torch.argmax(logits)\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()])"
      ],
      "metadata": {
        "id": "ZdJuV5xDEiF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "        r=8, lora_alpha=16, lora_dropout=0.05, task_type=\"SEQ_CLS\"\n",
        "    )"
      ],
      "metadata": {
        "id": "o263XKAUAiix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config"
      ],
      "metadata": {
        "id": "PcKpdYQOFdSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_dI2Xy04ZGVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir= runs_log_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=1,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        save_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        report_to=\"tensorboard\",\n",
        "        max_steps=-1,\n",
        "        push_to_hub=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "zdvoXAIYBEKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_arguments,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "metadata": {
        "id": "ulddTH99BI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrPJIhnFCfxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
        "\n",
        "gpus\n",
        "\n",
        "\n",
        "visible_devices = tf.config.get_visible_devices()\n",
        "for devices in visible_devices:\n",
        "  print(devices)\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "bpwbUNitndWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vR_V8vYNC-Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqqEZgdfivYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "f\"{dir_out}\""
      ],
      "metadata": {
        "id": "XD3MqCMoiv_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir_path = f\"{dir_out}/runs\"\n",
        "print (logdir_path)\n",
        "#huggingface_hub.upload_folder\n",
        "#/content/reeddg/training_outputs3/runs/Mar24_14-57-33_ecd96771620a\n",
        "%load_ext tensorboard\n",
        "#%tensorboard --logdir reeddg/training_outputs/04-07-05-45-10\n",
        "%tensorboard --logdir \"$dir_out\""
      ],
      "metadata": {
        "id": "T309SOcoi43t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save fine-tuned model\n",
        "ft_model_path = f\"reeddg/test_train/class_{model_fine_tuned}\"\n",
        "trainer.model.save_pretrained(ft_model_path)\n",
        "trainer.save_model(ft_model_path)\n",
        "trainer.push_to_hub(ft_model_path)"
      ],
      "metadata": {
        "id": "7FWRJiFgMsdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model on HuggingFace first"
      ],
      "metadata": {
        "id": "i4aRtg47OXMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reload base model back in\n",
        "base_model= AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "5J5y5cZKD0xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge base model and the fine-tuned model\n",
        "merged_model= PeftModel.from_pretrained(base_model, ft_model_path)\n",
        "merged_model= merged_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "z_rA7QXxEiA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save merged model\n",
        "merged_model_path= f\"reeddg/test_model/merged_{model_fine_tuned}\"\n",
        "merged_model.save_pretrained(merged_model_path)"
      ],
      "metadata": {
        "id": "ots2EPrTEsKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"reeddg/bert_test\")\n",
        "tokenizer.push_to_hub(\"reeddg/bert_test\")"
      ],
      "metadata": {
        "id": "_mfqcrngb-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define list of examples\n",
        "text_list = [\"The cleaner did not work well.\", \"I do not recommed.\", \"Tore and ripped.\", \"Very Good.\", \"This one fit well.\"]\n",
        "\n",
        "print(\"Trained model predictions:\")\n",
        "print(\"----------------------------\")\n",
        "for text in text_list:\n",
        "    # tokenize text\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    # compute logits\n",
        "    logits = merged_model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions = torch.argmax(logits)\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()])"
      ],
      "metadata": {
        "id": "-lxR8gFWehKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define list of examples\n",
        "text_list = [\"not good.\", \"Very good.\", \"faded\", \"Very Strong.\", \"Fit Great.\"]\n",
        "\n",
        "print(\"Untrained model predictions:\")\n",
        "print(\"----------------------------\")\n",
        "for text in text_list:\n",
        "    # tokenize text\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    # compute logits\n",
        "    logits = merged_model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions = torch.argmax(logits)\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()])\n",
        "    print(predictions.tolist())"
      ],
      "metadata": {
        "id": "ZATuyiBha-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model =AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=2, id2label=id2label, label2id=label2id, quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n"
      ],
      "metadata": {
        "id": "AmAPK0jsodwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\"reeddg/bert_test\", use_fast=True)\n",
        "#eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
        "\n",
        "def gen(model,p, maxlen=68, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    #res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cuda')\n",
        "    res = model.generate(**toks.to(\"cuda\"),max_new_tokens=maxlen,num_return_sequences=1,penalty_alpha=0.6,do_sample = True,\n",
        "      top_k=5,temperature=0.5,repetition_penalty=1.2,).to('cuda')\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uiB-v7vsIZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "\n",
        "qty_to_test=len(dataset['test'])\n",
        "dialogues = dataset['test'][0:qty_to_test]['text']\n",
        "baseline_ref = dataset['test'][0:qty_to_test]['labels']\n",
        "\n",
        "\n",
        "print(dialogues)\n",
        "print(baseline_ref)\n",
        "\n",
        "original_ref = []\n",
        "original_model_predict = []\n",
        "peft_model_predict = []\n",
        "time_to_respond_orig = []\n",
        "time_to_respond_peft = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    baseline_ref_output = baseline_ref[idx]\n",
        "\n",
        "\n",
        "    prompt = f\"{dialogue}\"\n",
        "    print(prompt)\n",
        "    inputs = eval_tokenizer.encode(dialogue, return_tensors=\"pt\")\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "\n",
        "    # compute logits\n",
        "    logits = original_model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions_o = torch.argmax(logits)\n",
        "    original_model_pred= (predictions_o.tolist())\n",
        "    print(original_model_pred)\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_orig.append(out_time)\n",
        "    print(f\"Time taken for original inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    start_time=perf_counter()\n",
        "\n",
        "\n",
        "    #inputs = tokenizer.encode(dialogue, return_tensors=\"pt\")\n",
        "    # compute logits\n",
        "    logits = merged_model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions_m = torch.argmax(logits)\n",
        "    peft_model_pred= (predictions_m.tolist())\n",
        "    print(peft_model_pred)\n",
        "\n",
        "    out_time = perf_counter() - start_time\n",
        "    time_to_respond_peft.append(out_time)\n",
        "    print(f\"Time taken for peft inference: {round(out_time,2)} seconds\")\n",
        "\n",
        "    original_model_predict.append(original_model_pred)\n",
        "    peft_model_predict.append(peft_model_pred)\n",
        "\n",
        "zipped_summaries = list(zip(baseline_ref, original_model_predict, peft_model_predict))\n",
        "zipped_time = list(zip(time_to_respond_orig, time_to_respond_peft))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['baseline_ref', 'original_model_predict', 'peft_model_predict'])\n",
        "df_time = pd.DataFrame(zipped_time, columns = ['orig_model_inference_time', 'peft_model_inference_time'])\n",
        "df"
      ],
      "metadata": {
        "id": "FMoGNIDGy9DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_results(results):\n",
        "    ref = results['baseline_ref']\n",
        "    p_pred = results['peft_model_predict']\n",
        "    o_pred = results['original_model_predict']\n",
        "\n",
        "    peft_a = accuracy.compute(predictions=p_pred, references=ref)\n",
        "    peft_r = recall.compute(predictions=p_pred, references=ref)\n",
        "    peft_p = precision.compute(predictions=p_pred, references=ref)\n",
        "    peft_f = f1.compute(predictions=p_pred, references=ref)\n",
        "    p = pd.array([peft_a['accuracy'], peft_r['recall'], peft_p['precision'], peft_f['f1']])\n",
        "\n",
        "    o_a = accuracy.compute(predictions=o_pred, references=ref)\n",
        "    o_r = recall.compute(predictions=o_pred, references=ref)\n",
        "    o_p = precision.compute(predictions=o_pred, references=ref)\n",
        "    o_f = f1.compute(predictions=o_pred, references=ref)\n",
        "    o = pd.array([o_a['accuracy'], o_r['recall'], o_p['precision'], o_f['f1']])\n",
        "\n",
        "    ret = pd.DataFrame([p,o],columns=[\"accuracy\", \"recall\", \"precision\",\"f1\"], index=[\"peft\", \"original\"])\n",
        "    return(ret)\n",
        "\n",
        "\n",
        "dataf=compare_results(df)\n",
        "print(dataf)"
      ],
      "metadata": {
        "id": "STPgOujq3dEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print (f\"Inference time peft: {round(np.average(time_to_respond_peft),4)}\")\n",
        "print (f\"Inference time original: {round(np.average(time_to_respond_orig),4)}\")\n",
        "\n",
        "time_p2o = np.subtract(time_to_respond_peft, time_to_respond_orig)\n",
        "time_2inf = np.average(time_p2o)\n",
        "time_p=[\"Time2Imp\"]\n",
        "#print(time_p)\n",
        "time_p = [*time_p, *time_p2o]\n",
        "#print (time_p)\n",
        "\n",
        "\n",
        "if(time_2inf>0):\n",
        "  print(f\"Inference time for original model is on average {round(time_2inf,2)} seconds better than the peft model.\")\n",
        "else:\n",
        "  print(f\"Inference time for peft model is on average {round(-time_2inf,2)} seconds better than the original model.\")"
      ],
      "metadata": {
        "id": "RIlcfqmOzIQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file= model_fine_tuned + \"class.csv\"\n",
        "\n",
        "dataf.to_csv(file, encoding='utf-8')\n",
        "\n",
        "file_time= \"time_\" + file\n",
        "\n",
        "df_time.to_csv(file_time,encoding='utf-8')\n",
        "\n"
      ],
      "metadata": {
        "id": "aAFjCiPaAm4o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}